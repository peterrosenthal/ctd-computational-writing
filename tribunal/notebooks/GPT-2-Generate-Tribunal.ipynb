{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GPT-2-Generate-Tribunal.ipynb","provenance":[{"file_id":"1L8egbur27j1vmn_s_zZsrKzIXNFopNdY","timestamp":1617828655980},{"file_id":"1mcpvytBDqu_9RsgA78ghRKPl93KYYgk0","timestamp":1617808522364}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rb54G188_U77"},"source":["The below command should produce a table of output, it just means that you do have a GPU instance, if you don't, go to \"Runtime\" and \"Change Runtime Type\" and make sure and use \"GPU\" hardware acceleration\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7eFpMy9f_PQQ","executionInfo":{"status":"ok","timestamp":1619474616860,"user_tz":360,"elapsed":418,"user":{"displayName":"Peter Rosenthal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir2AKFybPQBz02oH82uHm8MIrByqosFSWT57u_uQ=s64","userId":"03469292320161569337"}},"outputId":"0fcb2023-0ab7-426f-fe44-8a836b2bb38a"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qfRp9S9Cw875","executionInfo":{"status":"ok","timestamp":1619474616862,"user_tz":360,"elapsed":410,"user":{"displayName":"Peter Rosenthal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir2AKFybPQBz02oH82uHm8MIrByqosFSWT57u_uQ=s64","userId":"03469292320161569337"}},"outputId":"9806d2be-3707-4ded-8212-f7bac2aba63e"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CpDeznYICecY","executionInfo":{"status":"ok","timestamp":1619474617470,"user_tz":360,"elapsed":1010,"user":{"displayName":"Peter Rosenthal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir2AKFybPQBz02oH82uHm8MIrByqosFSWT57u_uQ=s64","userId":"03469292320161569337"}},"outputId":"1b81eaa9-c1a3-428d-92fa-702bc4613356"},"source":["!ln -sf /content/gdrive/MyDrive/tribunal/gpt2/models/ models\n","!ln -sf /content/gdrive/MyDrive/tribunal/gpt2/checkpoint/ checkpoint \n","!cd checkpoint ; ln -sf ../models/355M/ 355M\n","!ls -al checkpoint/355M/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dPSjjugC_mFl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619474621891,"user_tz":360,"elapsed":5425,"user":{"displayName":"Peter Rosenthal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir2AKFybPQBz02oH82uHm8MIrByqosFSWT57u_uQ=s64","userId":"03469292320161569337"}},"outputId":"18e06ed2-6d50-4a2e-d614-b7b45ec67ba5"},"source":["!pip3 install gpt-2-simple==0.7.1\n","!pip3 install tensorflow-gpu==1.15"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HQFnFhpuIbM6"},"source":["The following lines of python run inference on the model, generating output based on the prompt (prefix). 'length' is the amount of tokens (words) that are produced, 'temperature' makes the text more diverse the higher it is (0.1 less diverse text, 1.0 very diverse text) 'nsamples' is the amount of 'length' sized texts produced, 'batch_size' is the amount of samples created as a time (i.e if you had 'nsamples' = 100 and 'batch_size=20', the response would be 5 chunks of 20 'length' sized texts) 'top_k' restricts the amount of words the model will choose from for the next predicted word, if set to 0 this behavior is turned off, 'prefix' is the text you prompt the model with and 'include_prefix' includes the prefix in the output."]},{"cell_type":"code","metadata":{"id":"lXMTaBDBGsBF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619478004525,"user_tz":360,"elapsed":3388052,"user":{"displayName":"Peter Rosenthal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir2AKFybPQBz02oH82uHm8MIrByqosFSWT57u_uQ=s64","userId":"03469292320161569337"}},"outputId":"a1c471a2-7a91-4d2f-ce3e-e5cfaa22d051"},"source":["import gpt_2_simple as gpt2\n","import os\n","import requests\n","\n","base_fname = \"/content/gdrive/MyDrive/tribunal/gpt2/outputs/\"\n","model_name = \"run1\"\n","sess = gpt2.start_tf_sess()\n","gpt2.load_gpt2(sess, model_name)\n","\n","mylines = []\n","myOutput = []\n","\n","for i in range(0,100):\n","  mylines = gpt2.generate(sess, run_name=model_name,\n","\t  length=1024,\n","\t  temperature=0.9, #larger number means more diverse text\n","  \tnsamples=1, #you can generate multiple samples per run\n","\t  batch_size=1, #batch size must divide equally into nsamples\n","\t  top_k=0,\n","\t  prefix=\"<|startoftext|>\\n\",\n","\t  include_prefix=False,\n","    return_as_list=True,\n","    truncate='<|endoftext|>')\n","  if i % 25 == 0:\n","    # reset session\n","    gpt2.reset_session(sess)\n","    sess = gpt2.start_tf_sess()\n","    gpt2.load_gpt2(sess, model_name)\n","  fnum = i\n","  fname = base_fname + \"%04d\" % fnum + \".txt\"\n","  fh = open(fname, \"w\", encoding='utf-8')\n","  for line in mylines:\n","    fh.write(line)\n","  fh.close()\n","  print(\"Wrote file:\",fname)\n","\n","gpt2.reset_session(sess)\n","\n"],"execution_count":null,"outputs":[]}]}