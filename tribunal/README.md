# GPT-2 Plays League of Legends

This project is a recreation of a toxic chat log of a League of Legends game using the GPT-2 neural network.

## Data

The model was trained on a set of in-game chat logs, taken from the Tribunal, a no longer operational attempt at a crowd sourced player behavior management system. I downloaded the data from [Sim Sheng Xue on Kaggle](https://www.kaggle.com/simshengxue/league-of-legends-tribunal-chatlogs) who did not specify how they managed to get their hands on the data in the first place.

The data set was not included in this repository to avoid needing to use git LFS, but it can be downloaded from Kaggle (the link above), and imported into your local clone of this directory. From the downloaded data, the `chatlogs.csv` file, and the `input-data.json` file might be interesting to play with, though I only used the `.txt`s found under `cases/` (they're really JSONs).

Because of the nature of the Tribunal as a player behavior managment system, the chat log data collected from it represents a particularly spicy subset of chat experiences from the game. While League of Legends isn't a game particularly well known for its friendly community (it is distinctly known for the opposite), it's important to remember that your average chat experience each game isn't going to be as bad as this data set implies. I think that's a great thing for the purpose of fine tuning GPT-2 though, as the consistency of the behavior will make the output much more consistently spicy/angry/entertaining, however you'd like to put it.

I created the corpus to fine tune GPT-2 on out of the ~10,000 individual case files using the `parse_tribunal.py` script I wrote in this directory. There are no command line arguments to run with the program, so all you need to do is run `python3 parse_tribunal.py`, and it will transform the dataset into one single file `tribunal.txt`. There are two important differences between the raw data, and the `tribunal.txt` training corpus. The first is that `tribunal.txt` is marked up with GPT-2 `<|endoftext|>` tags and gpt-2-simple `<|startoftext|>` tags. The second difference is that `tribunal.txt` actually reads like a League of Legends chatbox would, which is important because GPT-2 is a *text* recreation neural network, not for raw data recreation or anything. There was one major issue with recreating chatbox-like text from the raw data, which is that the the "Summoner Names" (what other games/platforms would call the username or gamertag) were left out of the (publicly available) data for anonymity. But these Summoner Names are vital to the chatbox look as they are usually included with every single message. To get around this problem, I created a really basic name generator inspired by other crappy gamertag generators out there to assign random names to the players. This name generator is an area where the whole project can be vastly improved, as there's just not enough options to create any real diversity in the name pool, and that is very heavily reflected in the final output of the fine tuned GPT-2 model.

## Fine Tuning GPT-2 and Generating the Chat Logs

The fine-tuning of GPT-2 for this project was done on [Google Colab](https://colab.research.google.com). All the notebooks used are included in this repo under the `notebooks/` folder. To run them:
  * Create several folders in your Google Drive: `tribunal/`, `tribunal/gpt2/`, `tribunal/gpt2/checkpoint/`, `tribunal/gpt2/models/`, and `tribunal/gpt2/outputs/`.
  * Then upload all the notebooks from this repository to the `tribunal/` folder in your Google Drive, as well as the `tribunal.txt` training corpus.
  * First open `GPT-2-Get-Model-355M-Tribunal.ipynb` in Google Colab, and run all cells. This notebook will download the GPT-2 mid-sized base model to your Google Drive.
  * Then open and run `GPT-2-Tune-Tribunal.ipynb` in Google Colab, which will fine tune that base model using the tribunal.txt corpus.
  * Finally open and run `GPT-2-Generate-Tribunal.ipynb` in Googe Colab, which will fill your `tribunal/gpt2/ouptuts` folder in Google Drive with GPT-2 generated chat logs, for you to pick the best from!

## A Game of League of Legends, Generated by GPT-2

```
[00:17:24] [All] CandyGarlic (Caitlyn): i was warding noob
[00:17:32] [All] CandyGarlic (Caitlyn): lol
[00:17:46] [All] CandyGarlic (Caitlyn): lulu says we get noob cait
[00:18:00] CandyGarlic (Caitlyn): i am so op now
[00:18:11] [All] SadEgg (Lulu): lulu
[00:28:16] [All] CandyGarlic (Caitlyn): bot lane srsly lost
[00:28:33] [All] CandyGarlic (Caitlyn): gj
[00:28:38] [All] SadEgg (Lulu): same
[00:28:39] [All] SadEgg (Lulu): gj
[00:34:23] [All] XxBroccoliHippyxX (Renekton): omg
[00:35:15] [All] PleasantBank (Nami): ii
[00:35:20] [All] C9 TinyEnigma (Kayle): gg
[00:35:21] [All] SadEnigma (Caitlyn): lulu
[00:35:24] [All] C9 TinyEnigma (Kayle): gg wp
[00:35:25] [All] ToxicHospitalxX (Nami): gg
[00:35:26] [All] C9 TinyEnigma (Kayle): gg
[00:35:26] [All] C9 TinyEnigma (Kayle): bg
[00:35:27] [All] COisionsxx (Nasus): gg
```
From [0060.txt](outputs/0060.txt).

Some other of my favorites are [0006.txt](outputs/0006.txt), [0017.txt](outputs/0017.txt), [0038.txt](outputs/0038.txt), [0056.txt](outputs/0056.txt), [0059.txt](outputs/0059.txt), [0062.txt](outputs/0062.txt), [0070.txt](outputs/0070.txt), [0080.txt](outputs/0080.txt), [0083.txt](outputs/0083.txt), [0087.txt](outputs/0087.txt), and  [0096.txt](outputs/0096.txt).
